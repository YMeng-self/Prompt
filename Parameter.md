## 什么是 Prompt

Prompt 是在使用AI模型时为输入添加的一段文本或指令，以下是关于它的详细介绍：

1. **基本定义**
    - **文本或指令引导**：Prompt 是指在使用机器学习模型时为输入添加的一段文本或指令。其目的是为了引导模型生成更准确、更有针对性的输出。它可以是一个问题、一段描述、一种格式化的输入，甚至是一些关键词。通过在输入中添加 Prompt，我们可以对模型的行为进行定制和控制，使其更好地适应我们的需求。

2. **主要作用**
    - **解决数据偏差**：在机器学习中，数据的质量和多样性对于模型的性能至关重要。然而，由于数据采集的限制和偏差，我们常常面临着数据不平衡的问题。Prompt 可以通过引导模型生成特定类型的输出，从而在一定程度上解决数据偏差的问题。例如，在情感分类任务中，我们可以通过在输入中添加描述性的 Prompt，指导模型更好地理解情感的含义，从而减少情感分类错误的发生。
    - **提高模型可控性**：在实际应用中，我们通常需要对模型的输出进行控制，以确保其符合特定的要求。Prompt 可以通过给定特定的指令或约束，使模型生成满足我们期望的输出。例如，在机器翻译任务中，我们可以通过在输入中添加 “Translate from English to French:” 这样的 Prompt，告诉模型我们需要将英文翻译成法文。这种方式可以确保模型按照我们的要求进行翻译，提高翻译质量。
    - **辅助问题建模**：在问答系统或对话生成中，Prompt 可以帮助我们建模问题的形式和结构。通过合理设计 Prompt，我们可以引导模型理解问题的意图，并生成准确的回答。例如，在问答任务中，我们可以通过在输入中添加问题类型的 Prompt，如 “Who is the author of ”来指导模型回答与作者相关的问题。这样的设计能够增强模型对问题的理解和回答的准确性。

3. **常见类型**
    - **问题式 Prompt**：通过提出问题来引导模型生成答案。这类 Prompt 最直接、最简单，适用于需要获取具体信息的场景。例如：“谁是美国的第一任总统？”或“计算机科学的三个主要领域是什么？”问题式 Prompt 关注的是模型能否准确、快速地回答问题，因此在设计时要注意问题的表述清晰明确，以确保模型能够准确理解并作出相应回答。
    - **陈述式 Prompt**：通过描述一个场景或状态来引导模型生成相关内容。这类 Prompt 更注重模型的创造性和表现力，适用于需要模型产生描述、故事或观点等内容的场景。例如：“描述一幅美丽的日落画面。”或“谈谈你对全球变暖的看法。”陈述式 Prompt 关注的是模型生成的内容是否符合语境、具有连贯性和合理性。在设计时，要提供足够的背景信息和引导，以帮助模型生成更为丰富、精彩的内容。
    - **指令式 Prompt**：通过给出明确的指令来引导模型完成特定任务。这类 Prompt 要求模型具有较强的执行能力，适用于需要模型进行翻译、纠错、编程等任务的场景。例如：“将下面的英文短文翻译成中文。”或“修复下面 Python 代码中的语法错误。”指令式 Prompt 关注的是模型完成任务的准确性和效率。在设计时，要确保指令清晰、易懂，同时提供足够的信息以便模型能够成功完成任务。
    - **完形填空式 Prompt**：通过在文本中留出空白来引导模型生成符合语境的内容。这类 Prompt 更具挑战性，旨在测试模型的理解能力和逻辑推理能力。例如：“今天是个晴朗的日子，适合去_____。”或“苹果手机的操作系统是_____。”完形填空式 Prompt 关注的是模型生成的内容是否符合语境、逻辑严密。在设计时，要注意空白处的位置和数量，以确保模型能够根据上下文推断出正确的答案。


4. **设计原则**
    - **清晰明确**：确保 Prompt 表达清晰，易于理解。避免模棱两可或模糊不清的表述，这样才能让模型更好地理解我们的需求。在设计时，要注意使用简单、直接的语言，避免过于复杂或专业的词汇，以免增加模型的理解难度。
    - **精简高效**：尽量使用简洁、精练的语言。过长或冗余的 Prompt 可能会导致模型理解困难，影响输出结果。在设计时，要注意去除不必要的信息，保留关键点，使 Prompt 简洁明了，便于模型快速处理。
    - **上下文连贯**：确保 Prompt 与上下文语境一致，这有助于模型生成更符合预期的回答。在设计时，要考虑 Prompt 与前后文的关联性，使其内容和逻辑紧密相连，以便模型能够根据上下文生成恰当的输出。
    - **针对性强**：根据应用场景和目标用户来设计 Prompt，使其更具针对性。了解用户需求和应用场景，以便为不同需求量身定制 Prompt，从而提高模型输出的准确性和实用性。
    - **可迭代优化**：将 Prompt 设计视为一个持续迭代的过程。通过多次实验和测试，收集模型输出的反馈，不断优化和调整 Prompt。在每次迭代中，根据模型的输出情况，调整 Prompt 的内容和结构，以提高性能。



## 角色

在人工智能（AI）的交互过程中，不同的角色承担着不同的功能和任务。以下是 AI 的 user、assistant 和 system 等角色的介绍：

1. **user（用户角色）**
   - **定义**：代表实际的最终用户，是与 AI 模型互动的人，向 ChatGPT 发送提示或消息。
   - **作用**：使用 “user” 角色发送消息时，是在模拟用户在对话中的输入，AI 模型会将该消息解释为来自用户，并相应地生成响应。

2. **assistant（助手角色）**
   - **定义**：代表 AI 模型本身，即响应最终用户提示的实体，其消息包含由 AI 生成的内容，作为对用户输入的响应。
   - **作用**：用于在当前请求中设置模型的先前响应，以保持对话的连贯性。当 API 返回响应时，会包括一个 “助手” 角色的消息，该消息包含由 AI 生成的内容，作为对用户输入的响应。

3. **system（系统角色）**
   - **定义**：用于为聊天助手分配特定行为或上下文，指示 AI 在对话消息中应该具有哪种个性。
   - **作用**：通过传递带有系统角色的消息，可以创建具有不同个性、专长或行为的 AI 助手，从而使 AI 的响应与特定的使用场景相匹配。例如，可以将聊天助手设置为体育专家、无礼的助手或乐于助人且礼貌的助手等。



## 参数

这些参数通常用于配置生成文本的模型，以下是每个参数的解释：

1. `stream: False`
   - 是否以流的形式返回结果。如果设置为True，模型会逐步生成文本并实时返回；如果为False，则等待整个文本生成完毕后再一次性返回。

2. `max_tokens: 512`
   - 生成文本的最大长度（以token为单位）。一个token通常对应一个单词或标点符号。

3. `stop: ["null"]`
   - 停止生成的条件。当模型生成到指定的字符串时，会停止生成。在这个例子中，"null"是一个占位符，表示没有特定的停止条件。

4. `temperature: 0.7`
   - 控制生成文本的随机性。较高的值（接近1）会使输出更加随机和多样化，较低的值（接近0）会使输出更加确定和保守。

5. `top_p: 0.7`
   - 核采样方法中的参数，用于控制生成文本的多样性。它指定了选择下一个词的概率分布的累积概率阈值。例如，top_p=0.7意味着只考虑累计概率达到70%的词。

6. `top_k: 50`
   - 另一种控制生成文本多样性的方法。它指定了在生成下一个词时，只考虑前k个最可能的词。在这个例子中，k=50。

7. `frequency_penalty: 0.5`
   - 频率惩罚系数。用于减少生成文本中重复出现的词。较高的值会增加对重复词的惩罚，使生成的文本更多样化。

8. `n: 1`
   - 生成文本的数量。在这个例子中，n=1表示只生成一个文本。

这些参数共同作用，决定了生成文本的长度、多样性、随机性和特定条件下的停止条件。通过调整这些参数，可以控制生成文本的特性，以满足不同的应用需求。



## Temperature 设置

DeepSeek `temperature` 参数默认为 1.0。推荐按使用场景设置 `temperature`。

|场景|温度|
|---|---|
|代码生成/数学解题|0.0|
|数据抽取/分析|1.0|
|通用对话|1.3|
|翻译|1.3|
|创意类写作/诗歌创作|1.5|

